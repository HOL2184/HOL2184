{"cells": [{"metadata": {"id": "815dd0a8-71fc-4afb-9568-7b7fbff91fec"}, "cell_type": "markdown", "source": "#### HOL2184 - lab 4 - Data Virtualization"}, {"metadata": {"id": "c93e6cc7-de51-4a75-b761-4d0240a269ff"}, "cell_type": "markdown", "source": "## 1. Introduction\n\nWelcome to the lab for Data Virtualization. \n\nIn this lab you analyze data from multiple data sources, without copying data.\n\nThis hands-on lab uses data from 4 data sources, were data is \u201cvirtually\u201d available through the IBM Cloud Pak for Data Virtualization Service. This would make it easy to analyze data from across your multi-cloud enterprise using tools like, Jupyter Notebooks, Watson Studio or your favorite reporting tool like Cognos.  \n"}, {"metadata": {"id": "c3a15ef7aaa3465d827c2d4ff24ce5ae"}, "cell_type": "markdown", "source": "## 2. Exploring Data Source Connections\nLet's start by looking at the the Data Source Connections that are available in this environment. \n\n1. Click the three bar (hamburger) menu at the top left of the console\n2. Click on the Data menu item if is not already expanded\n3. Right click **Data Virtualization** and select **Open in New Window**\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV02.png\"><br>\n4. Click on the submenu (**Virtualize**) and select **Data Sources** to show the currently defined data source.\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV03.png\"><br>\n4. Click **Constellation View**. A spider diagram of the connected data sources opens. \n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV04.png\"><br>\n\n    This displays the Data Source Graph with a number of active data sources:\n    * 4 Db2 Family Databases hosted on premises, on Cloud Pak for Data and on OpenShift on AWS\n    * 1 Remote connector\n    * 1 MongoDB Enterprise data server running as a Cloud Pak for Data service and on Premises\n    * 1 Enterprise DB Postgres data server running on premises and on Cloud Pak for Data\n    * 1 Netezza Performance Server (using the Pure Data for Analyics connection) running on the Cloud\n    * 1 MySQL data server running on premises\n    * 1 Informix Database running on premises \n    * 1 BigSQL engine running as a Cloud Pak for Data service\n    * 1 file system on a remote server\n"}, {"metadata": {"id": "ab5aaa7ceec04f4e81227693b10fb6c1"}, "cell_type": "markdown", "source": "## 3. Virtualize tables\n\nThe data sources are already defined for this lab. Now we want to virtualize tables so that we can run queries over several data sources. IBM Cloud Pak for Data searches through the available data sources and compiles a single large inventory of all the tables and data available to virtualize in IBM Cloud Pak for Data. \n\n1. Click the Data Virtualization menu and select **Virtualize** under **Virtualization**\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV05.png\"><br>\n    \n2. Check the total number of available tables at the top of the list. There should be hundreds available. We now virtualize our tables from various data sources. You can type the name of a table in the search field or reduce the number of tables by restricting the data source type. We start with the **ORDERS** table.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV06.png\"><br>\n\n3. Find the **ORDRES** table in data source **BigSQL** with Schema **USER00**. You can preview the clicking the eye on the right. Select the entry by checking the box on the left and press **Add to cart**. You know see on the top right that the first item is in the shopping cart.\n\n\n4. We will now add more tables to the shopping cart. Follow the same process also for these tables:\n\n- Data source **Db2 Warehouse on Cloud Pak for Data**\n    - USER.CUSTOMER\n    - USER.LINEITEM\n    - USER.REGION\n    - USER.NATION\n- Data source **EDB Postgres on Cloud Pak for Data**\n    - public.supplier\n- Data source **MongoDB orders**\n    - \"ORDERS-DATABASE.PARTSUPP-COLLECTION\n    \n5. We now should have 7 items in the cart. Now we add a file as a table. Select **File** from the tab above.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV22.png\"><br>\n    \n6. Open the file server **server7**.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV23.png\"><br>\n\n7. Select the directory **additionalData** and select the file **part.csv**.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV24.png\"><br>\n\n8. Add the file also to the cart. We know should have 8 items in the cart. Open the cart by clicking **View cart(8)**\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV25.png\"><br>\n\n9. In the cart are 8 items and some have to be changed. But first select **My virtualized data** as a target. Then change the names of the following items\n    - supplier \u21d2 SUPPLIER\n    - PARTSUPP-COLLECTION \u21d2 PARTSUPP\n    - part_csv \u21d2 PART\n<br><br>\n    \n10. For some table we have to change the columns. Click on the 3 dots on the right for table **SUPPLIER** and select **Edit columns**. \n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV09.png\"><br>\n\n11. Change all column names to uppercase for this table and click on **APPLY**\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV10.png\"><br>\n    \n12. Click **Edit columns** for **PARTSUPP** and deselect the columns **INDEX** and **_ID**. \n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV11.png\"><br>\n\n13. We are now ready to virtualize the tables. Click on **Virtualize** to do so.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV12.png\"><br>\n\n14. After the virtualization has finished execution you can click on **View my virtualized data**. \n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV13.png\"><br>\n\n15. The table shows the virtualized tables. There might be more tables than you published here as you can also see virtualized tables others made available to you. You can filter by your username to see the tables created in the previous steps.\n\n    <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV14.png\"><br>\n"}, {"metadata": {"id": "b3325e5cd650475c833b9bf8a836df14"}, "cell_type": "markdown", "source": "# 4. Access DV from SQL\n\nData Virtualization in Cloud Pak for Data is behaving like a database. It is offering a JDBC interface to run SQL and it is using the Db2 drivers. \n\nThe next part of the lab relies on a Jupyter notebook extension, commonly refer to as a \"magic\" command, to connect to a Db2 database. To use the commands you load load the extension by running another notebook call db2 that contains all the required code \n<pre>\n&#37;run db2.ipynb\n</pre>\nThe cell below loads the Db2 extension directly from GITHUB. Note that it will take a few seconds for the extension to load, so you should generally wait until the \"Db2 Extensions Loaded\" message is displayed in your notebook. \n1. Click the cell below\n2. Click **Run**. When the cell is finished running, In[*] will change to In[2]"}, {"metadata": {"id": "112fe3994ae54f3f8ae983a2f4ec47ea"}, "cell_type": "code", "source": "# !wget https://raw.githubusercontent.com/IBM/db2-jupyter/master/db2.ipynb\n!wget -O db2.ipynb https://raw.githubusercontent.com/Db2-DTE-POC/CPDDVLAB/master/db2.ipynb\n\n%run db2.ipynb\nprint('db2.ipynb loaded')", "execution_count": null, "outputs": []}, {"metadata": {"id": "e3db52e9c67043de82e023150d91d2cb"}, "cell_type": "markdown", "source": "### 4.1 Gaining Insight from Virtualized Data\n\nTo connect to the data virtualization engine we have to speccify user and password. Please change the values below for your assigned lab user. After changing the values click on **Run** above or press **Shift-Enter** to execute the code cell"}, {"metadata": {"id": "b0dfc3f0ca964dfdb9d5757168009a64"}, "cell_type": "code", "source": "# Connect to the IBM Cloud Pak for Data Virtualization Database from inside CPD\n\nuser = \"USERxx\"\npassword = 'HOL2184'\n\ndatabase = 'bigsql'\nhost = '10.1.1.1'\nport = '32601'\n\n%sql CONNECT TO {database} USER {user} USING {password} HOST {host} PORT {port}", "execution_count": null, "outputs": []}, {"metadata": {"id": "f5799962a4fe4295936e2194627adcbb"}, "cell_type": "markdown", "source": "Now that you are connected to the Data Virtualization engine you can query the virtualized tables using all the power in the Db2 SQL query engine. \n\nWe have the 8 tables that we virtualized before available for querying:\n\n- SUPPLIER\n- PART\n- PARTSUPP\n- CUSTOMER\n- NATION\n- REGION\n- LINEITEM\n- ORDERS\n\nWe start with a query for the suppliers."}, {"metadata": {"id": "0b3d8144bf534f388407c7b31420090d"}, "cell_type": "code", "source": "%sql select * from SUPPLIER FETCH FIRST 5 ROWS ONLY", "execution_count": null, "outputs": []}, {"metadata": {"id": "9d16e7c416ab478882449c387881cbd6"}, "cell_type": "markdown", "source": "We can also run a query that joins tables from multiple sources. Run the cell below."}, {"metadata": {"id": "21ddb89f3efa430493411530a9aa6935"}, "cell_type": "markdown", "source": "Next, we are running a more complex query that includes data from multiple data sources. This query finds which supplier should be selected to place an order for a given part in a given region. It will run a few seconds (>15s), so you have to be patient. \n\nWhile you are waiting you look at the included tables and see above in which data sources they are located."}, {"metadata": {"id": "2cb41b2831054b8b82916e8def3386ab"}, "cell_type": "code", "source": "%%time\n\n%%sql -a\nselect\n    s_acctbal,\n    s_name,\n    n_name,\n    p_partkey,\n    p_mfgr,\n    s_address,\n    s_phone,\n    s_comment\nfrom\n    part,\n    supplier,\n    partsupp,\n    nation,\n    region\nwhere\n    p_partkey = ps_partkey\n    and s_suppkey = ps_suppkey\n    and p_size = 15\n    and p_type like '%BRASS'\n    and s_nationkey = n_nationkey\n    and n_regionkey = r_regionkey\n    and r_name = 'EUROPE'\n    and ps_supplycost = (\n        select\n            min(ps_supplycost)\n    from\n            partsupp,\n            supplier,\n            nation,\n            region\n        where\n            p_partkey = ps_partkey\n            and s_suppkey = ps_suppkey\n            and s_nationkey = n_nationkey\n            and n_regionkey = r_regionkey\n            and r_name = 'EUROPE'\n    )\norder by\n    s_acctbal desc,\n    n_name,\n    s_name,\n    p_partkey;", "execution_count": null, "outputs": []}, {"metadata": {"id": "2bddfd91efb240d38fea074da111cccc"}, "cell_type": "markdown", "source": "Data Virtualization has a explain tool that can show you how the query is processed. The above query has the following plan:\n\n   <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV19.png\"><br>\n\nAs the size of the plan is huge the following snippet gives you an impression what operations are done while calculating the query result:\n\n   <img src=\"https://raw.githubusercontent.com/HOL2184/HOL2184-resources/main/40-Data_Virtualization/images/DV21.png\"><br>\n"}, {"metadata": {"id": "b646a1af718843298cb826fbe8714c26"}, "cell_type": "markdown", "source": "### 4.2 Seeing where your Virtualized Data is coming from\nYou may eventually work with a complex Data Virtualization schema with dozens or hundres of data sources. As an administrator or a Data Scientist you may need to understand where data is coming from. \n\nThe following query shows the tables available for your user schema."}, {"metadata": {"id": "3d2de312330641238fd00be0dd474fde"}, "cell_type": "code", "source": "%%sql -a\nSELECT TABSCHEMA, TABNAME\n  FROM SYSCAT.NICKNAMES\n    WHERE TABSCHEMA = :user\n    ORDER BY TABSCHEMA, TABNAME", "execution_count": null, "outputs": []}, {"metadata": {"id": "b8d4fa50967c48f58f644e8617bff732"}, "cell_type": "markdown", "source": "If you want to know details about the source of aquery, the following query returns this information for the table **PARTSUPP**."}, {"metadata": {"id": "c1ae541ff1744fd78f1d377028ad2b93"}, "cell_type": "code", "source": "%%sql -a \nselect * from table(dvsys.GET_VT_SOURCES(:user, 'PARTSUPP'))", "execution_count": null, "outputs": []}, {"metadata": {"id": "0aa2ec88b3e4453284b8357c2c1cbbbd"}, "cell_type": "markdown", "source": "You can also find the same information in the Cloud Pak for Data user interface. Look for the **Metadata** option in the interface.\n\nTo see the source of all your virtual tables we just need to join the query and the procedure call."}, {"metadata": {"id": "dd50877261d047269ec4e734a00bc1e5"}, "cell_type": "code", "source": "%%sql -a\nSELECT N.TABSCHEMA AS TABSCHEMA, N.TABNAME AS TABNAME, S.SRCTABNAME AS SRCTABNAME, S.SRCSCHEMA AS SRCSCHEMA, S.SRCTYPE AS TYPE, S.DRIVER AS DRIVER, S.URL AS URL, S.USER AS USER, S.HOSTNAME AS HOSTNAME, S.PORT AS PORT, S.DBNAME AS DBNAME\n  FROM SYSCAT.NICKNAMES N, TABLE(\n  DVSYS.GET_VT_SOURCES(N.TABSCHEMA, N.TABNAME)) S\n  WHERE N.TABSCHEMA = :user", "execution_count": null, "outputs": []}, {"metadata": {"id": "c69589a645bb4445961117970fef0774"}, "cell_type": "markdown", "source": "This concludes lab 4."}, {"metadata": {"id": "50a75509189c459ea859684f5d229b3f"}, "cell_type": "markdown", "source": "**This project contains Sample Materials, provided under license. <br>\nLicensed Materials - Property of IBM. <br>\n\u00a9 Copyright IBM Corp. 2021. All Rights Reserved. <br>\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.<br>**"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}